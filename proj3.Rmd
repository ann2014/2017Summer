---
title: "Project 3 - Data 643"
author: "Ann Liu-Ferrara"
date: "June 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## singular value decomposition (SVD) 

. SVD builds features that may or may not map neatly to items (such as movie genres or news
topics). As in many areas of machine learning, the lack of explainability can be an issue).
. SVD requires that there are no missing values. There are various ways to handle this, including
(1) imputation of missing values, (2) mean-centering values around 0, or (3) <advanced> using
a more advance technique, such as stochastic gradient descent to simulate SVD in populating
the factored matrices.
. Calculating the SVD matrices can be computationally expensive, although calculating ratings
once the factorization is completed is very fast. You may need to create a subset of your data
for SVD calculations to be successfully performed, especially on a machine with a small RAM
footprint.

```{r}
if("recommenderlab" %in% rownames(installed.packages()) == FALSE){
  install_github("mhahsler/recommenderlab")
}

library(recommenderlab)
library(devtools)
install_github(repo = "SVDApproximation", username = "tarashnot")
library(SVDApproximation)

dim(ratings)
head(ratings)
summary(ratings)
visualize_ratings(ratings_table = ratings)

set.seed(1)
mtx <- split_ratings(ratings_table = ratings, 
                     proportion = c(0.7, 0.15, 0.15))

model <- svd_build(mtx)

model_tunes <- svd_tune(model, r = 2:50)

model_tunes$train_vs_valid

rmse_svd <- svd_rmse(model, r = model_tunes$r_best, rmse_type = c("test"))
rmse_svd


```

# Alternating Least Squares (ALS)

```{r}
als <- Recommender(data = MovieLense[1:500,], method = "ALS", 
                 parameter = list(normalize=NULL, lambda=0.1, n_factors=10, 
                                  n_iterations=10, seed = NULL, verbose = FALSE)) 
recom_ratings <- predict(als, newdata = MovieLense[501:502,], type = "ratings")
as(recom_ratings, "matrix")[501:502]
recom_topNList <- predict(als, newdata = MovieLense[501:502,], type = "topNList", n = 5)
as(recom_topNList, "list")

scheme <- evaluationScheme(MovieLense, method="split", train=0.9, given=-5, goodRating=4)

accuracy_table <- function(scheme, algorithm, parameter){
  als <- Recommender(getData(scheme, "train"), algorithm, parameter = parameter)
  p <- predict(als, getData(scheme, "known"), type="ratings")                      
  acc_list <- calcPredictionAccuracy(p, getData(scheme, "unknown"))
  total_list <- c(algorithm =algorithm, acc_list)
  total_list <- total_list[sapply(total_list, function(x) !is.null(x))]
  return(data.frame(as.list(total_list)))
}

table_random <- accuracy_table(scheme, algorithm = "RANDOM", parameter = NULL)
table_ubcf <- accuracy_table(scheme, algorithm = "UBCF", parameter = list(nn=50))
table_ibcf <- accuracy_table(scheme, algorithm = "IBCF", parameter = list(k=50))
table_pop <- accuracy_table(scheme, algorithm = "POPULAR", parameter = NULL)
table_ALS_1 <- accuracy_table(scheme, algorithm = "ALS", 
                              parameter = list( normalize=NULL, lambda=0.1, n_factors=200, 
                                                n_iterations=10, seed = 1234, verbose = TRUE))

rbind(table_random, table_pop, table_ubcf, table_ibcf, table_ALS_1)

algorithms <- list("random items" = list(name="RANDOM", param=NULL),
                   "popular items" = list(name="POPULAR", param=NULL),
                   "user-based CF" = list(name="UBCF", param=list(nn=50)),
                   "item-based CF" = list(name="IBCF", param=list(k=50)),
                   "SVD approximation" = list(name="SVD", param=list(k = 50)),
                   "ALS_explicit" = list(name="ALS", 
                      param = list(normalize=NULL, lambda=0.1, n_factors=200, 
                                   n_iterations=10, seed = 1234, verbose = TRUE)),
                   "ALS_implicit" = list(name="ALS_implicit", 
                      param = list(lambda=0.1, alpha = 0.5, n_factors=10, 
                                   n_iterations=10, seed = 1234, verbose = TRUE))
)

results <- evaluate(scheme, algorithms, type = "topNList", n=c(1, 3, 5, 10, 15, 20))
avg(results)

plot(results, annotate=c(1,3), legend="topleft")

```

# Matrix Factorization with GD

```{r}
set.seed(1)
in_train <- rep(TRUE, nrow(ratings))
in_train[sample(1:nrow(ratings), size = round(0.2 * length(unique(ratings$user)), 0) * 5)] <- FALSE

ratings_train <- ratings[(in_train)]
ratings_test <- ratings[(!in_train)]

write.table(ratings_train, file = "trainset.txt", sep = " ", row.names = FALSE, col.names = FALSE)
write.table(ratings_test, file = "testset.txt", sep = " ", row.names = FALSE, col.names = FALSE)

r = Reco()

opts <- r$tune("trainset.txt", opts = list(dim = c(1:20), lrate = c(0.05),
                                     nthread = 4, cost = c(0), niter = 200, nfold = 10, verbose = FALSE))

r$train("trainset.txt", opts = c(opts$min, nthread = 4, niter = 500, verbose = FALSE))

outfile = tempfile()

r$predict("testset.txt", outfile)
  
scores_real <- read.table("testset.txt", header = FALSE, sep = " ")$V3
scores_pred <- scan(outfile)
  
rmse_mf <- sqrt(mean((scores_real-scores_pred) ^ 2))
rmse_mf


```

# Slope One

```{r}
names(ratings) <- c("user_id", "item_id", "rating")
ratings <- data.table(ratings)

ratings[, user_id := as.character(user_id)]
ratings[, item_id := as.character(item_id)]

setkey(ratings, user_id, item_id)

set.seed(1)

in_train <- rep(TRUE, nrow(ratings))
in_train[sample(1:nrow(ratings), size = round(0.2 * length(unique(ratings$user_id)), 0) * 5)] <- FALSE

ratings_train <- ratings[(in_train)]
ratings_test <- ratings[(!in_train)]

ratings_train_norm <- normalize_ratings(ratings_train)

model <- build_slopeone(ratings_train_norm$ratings)

predictions <- predict_slopeone(model, 
                                ratings_test[ , c(1, 2), with = FALSE], 
                                ratings_train_norm$ratings)
unnormalized_predictions <- unnormalize_ratings(normalized = ratings_train_norm, 
                                                ratings = predictions)

rmse_slopeone <- sqrt(mean((unnormalized_predictions$predicted_rating - ratings_test$rating) ^ 2))
rmse_slopeone



```

# Summary of findings and recommendations

Summary of findings and recommendations
Compare the RMSEs of content based model, user-user based model, and item-item based model, they are 0.9578232, 0.985047, 1.032426 respectively. The results show that the model accuracy from high to low is content based model, user-user based model, item-item based model. Generally speaking, the highest performance of these three models is item-item based model, and the content based is the lowest. It looks a conflict of the results with the general model performance.
I would consider to rerun the model at least 10 times to get a more accurate result.


# Reference:

Building a Recommendation System with R by Suresh K. Gorakala, Michele Usuelli

https://rpubs.com/tarashnot/recommender_comparison


